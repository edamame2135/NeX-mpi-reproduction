---
layout: post
comments: true
title: NeX novel view synthesis
author: Kevin Tang, Edmond Xie
date: 2022-01-28
---


> Novel view synthesis aims to generate a visual scene representation from just a sparse set of images and has a wide variety of uses in VR/AR. In this blog, we will explore a new approach to this problem called NeX.



<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Introduction
We will explore NeX, a new method of novel view synthesis that uses enhanced multiplane image (MPI) techniques to produce view-dependent effects in real-time.

![nex 1]({{ '/assets/images/team16/lobstah.gif' | relative_url }})
{: style="height:400px; width: 500px; max-width: 100%;"}
![nex 2]({{ '/assets/images/team16/schweets.gif' | relative_url }})
{: style="height:400px; width: 500px; max-width: 100%;"}
*Example videos from the paper* [1].

## Technical Details
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NeX aims to improve upon a previous approach to view synthesis called multiplane image (MPI) [2]. MPI uses deep neural networks to extrapolate new views with just two stereo input images. The network takes in as input the two input images as well as camera parameters and infers a global scene representation. The scene is represented as a set of planes, each of which represents an RGB color image and $$\alpha$$/transparency map at a certain depth. In each layer the $$\alpha$$ map is used to occlude certain pixels based on the depth. Then, these planes can then be used to synthesize novel views using homography and composition functions.<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;While MPI is a more effective solution to view synthesis than previous methods, it still had some issues. Namely, the RGB$$\alpha$$ representation works best on diffuse surfaces and struggles to capture view dependent effects like reflections and specular highlights. MPI and similar methods also suffer from expensive inference which prohibit real-time rendering.<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NeX solves this problem by parameterizing each color value in the MPI as a function of the viewing angle instead of storing color as a static value. This function is approximated as a linear combination of learnable spherical basis functions: $$ C^p(v) = k^p_0 + \sum_{n=1}^{N}k^p_nH_n(v) $$ where $$k^p_n$$ for pixel $$p$$ is the RGB coefficient/reflectence parameters for the $$N$$ basis functions denoted by $$H_n(v)$$. Two separate Multilayer perceptrons (MLPs) are used to optimize the parameters and basis functions.<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The first MLP predicts per-pixel parameters given the pixel location:
$$
F_\theta : (x) \rightarrow (\alpha,k_1,k_2,...,k_N)
$$
where $$x = (x, y, d)$$ contains the location information of the pixel $$(x, y)$$ at depth $$d$$. 
The second MLP is used for predicting all global basis functions given the viewing angle. This second MLP is kept separate to ensure that the basis functions are not functions of pixel location and is modeled by:
$$
G_\phi: (v) \rightarrow (H_1,H_2,...,H_N)
$$ where $$v$$ is the normalized viewing direction.
<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It should be noted that for $$F_\theta$$, the first coefficient, $$k_0$$, is not learned but instead stored explicitiy so as to keep a "base color" for each pixel and help retain finer details while also reducing computation cost. Another optimization done is sharing coefficients across multiple planes instead of computing different coefficients for each individual pixel, which results in significant speed gain without compromising visual quality. To optimize the model, the two MLP's are evaluated to generate an output image which is then compared with a ground truth image at the same view. The loss is given as follows:
$$
L_{rec}(\hat{I_i}, I_i) = \|\hat{I_i} - I_i\|^2 + \omega \|\nabla\hat{I_i} - \nabla I_i\|_1 
$$
where $$\hat{I_i}$$ is the reconstructed image and $$I_i$$ is the ground truth image.
<br/>


## Algorithm
![nex 2]({{ '/assets/images/team16/train_alg.png' | relative_url }})
{: style="height:400px; width: 500px; max-width: 100%;"}
<br/>
*Algorithm from the paper* [1].
<br/>
<br/>
To summarize, first compute pixel location information for each pixel and for each iteration, compute $$\alpha$$ and pixel parameters $$\vec{K}$$ with $$F_\theta$$. Then, compute the viewing angle $$v$$ and evaluate $$G_\phi$$ to get the basis functions $$\vec{H}_{phi}(v)$$. Get the color functions $$C$$ via linear combination of the $$\vec{K}$$ and $$\vec{H}_{phi}(v)$$ and compose the image using $$\alpha$$ and $$C$$. Finally, compute the loss and optimize.


## Implementation
First the images are preprocced with COLMAP [4] structure-from-motion algorithm in order to calibrate the images. 
<br/>
![]({{ '/assets/images/team16/mlp1.png' | relative_url }})
{: style="height:400px; width: 750px; max-width: 100%;"}

For computing $$\alpha$$ and pixel parameters with $$F_\theta$$, the pixel location input $$(x, y)$$ takes 40 dimensions while the depth d takes 16 dimensions, totalling the input to 56 dimensions. The network consists of 6 fully connected layers with LeakyReLU activation, each with 384 nodes. The final output consists of $$\alpha$$, which uses sigmoid activation, and $$\vec{K}$$, which uses $$tanh$$ activation. 
<br/>
![]({{ '/assets/images/team16/mlp2.png' | relative_url }})
{: style="height:550px; width: 750px; max-width: 100%;"}

<br/>
<br/>
$$G_\phi$$ takes in as input a 12 dimension input for viewing direction and consists of 3 fully connected layers with LeakyReLU activation, each with 64 nodes. The 8-dimensional output are the basis functions $$\vec{H}_{phi}(v)$$.

## Demo 

## Reference

[1] Wizadwongsa, Suttisak, et al. "NeX: Real-time View Synthesis with Neural Basis Expansion" *arXiv preprint arXiv:2103.05606.* (2021).

[2] Zhou, Tinghui, et al. "Stereo Magnification: Learning view synthesis using multiplane images" *arXiv preprint arXiv:1805.09817* (2018).

[3] Ben Mildenhall, et al. "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis" *Proceedings of ECCV conference* (2020).

[4] Johannes Lutz Schonberger and Jan-Michael Frahm. "Structure-from-motion revisited." In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 

## Code Repository
[1] [NeX](https://github.com/nex-mpi/nex-code/)

[2] [Stereo Magnification](https://github.com/google/stereo-magnification)

[3] [NerF](https://github.com/bmild/nerf)

---
